[2025-08-11 16:07:11 base_config.py line 213]=>INFO: Initializing main logger ...
[2025-08-11 16:07:11 base_config.py line 217]=>INFO: Setting fixed seed: 131
[2025-08-11 16:07:13 base_config.py line 258]=>INFO: Collecting system info ...
[2025-08-11 16:07:13 base_config.py line 259]=>INFO: Project configuration:
DATALOADER:
  NUM_WORKERS: 0
  TEST:
    BATCH_SIZE: 1
  TRAIN:
    BATCH_SIZE: 1
DATASET:
  AUDIO_PATH: wav
  NAME: Vocaset
  READ_AUDIO: True
  ROOT: ./data/
  SPLIT:
    BIWI:
      TEST: [37, 38, 39, 40]
      TRAIN: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]
      VAL: [33, 34, 35, 36]
    VOCASET:
      TEST: [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
      TRAIN: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
      VAL: [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
  TEMPLATE_FILE: templates.pkl
  TEST_SUBJECTS: ['FaceTalk_170809_00138_TA', 'FaceTalk_170731_00024_TA']
  TRAIN_SUBJECTS: ['FaceTalk_170728_03272_TA', 'FaceTalk_170904_00128_TA', 'FaceTalk_170725_00137_TA', 'FaceTalk_170915_00223_TA', 'FaceTalk_170811_03274_TA', 'FaceTalk_170913_03279_TA', 'FaceTalk_170904_03276_TA', 'FaceTalk_170912_03278_TA']
  VAL_PERCENT: 0.1
  VAL_SUBJECTS: ['FaceTalk_170811_03275_TA', 'FaceTalk_170908_03277_TA']
  VERTICES_PATH: vertices_npy
ENV:
  DESCRIPTION: Train the model of stage 1 on the vocaset dataset.
  GPU: 0
  NAME: train
  OUTPUT_DIR: ./output
  RESUME: 
  SEED: 131
  USE_CUDA: True
  VERBOSE: True
  VERSION: 1
INPUT:
  GN_MEAN: 0.0
  GN_STD: 0.15
  NO_TRANSFORM: False
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
LOSS:
  NAME: VQLoss
  QUANT_LOSS_WEIGHT: 1.0
MODEL:
  BACKBONE:
    FACE_QUAN_NUM: 16
    HIDDEN_SIZE: 1024
    INAFFINE: False
    INTERMEDIATE_SIZE: 1536
    IN_DIM: 15069
    NAME: stage1_vocaset
    NEG: 0.2
    NUM_ATTENTION_HEADS: 8
    NUM_HIDDEN_LAYERS: 6
    PRETRAINED: True
    QUANT_FACTOR: 0
    WINDOW_SIZE: 1
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
    N_EMBED: 256
    ZQUANT_DIM: 64
  INIT_WEIGHTS: 
  NAME: 
  WAV2VEC2_PATH: facebook/wav2vec2-base-960h
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  ADAPTIVE_LR: False
  BASE_LR_MULT: 0.1
  FACTOR: 0.3
  GAMMA: 0.5
  LR: 0.0001
  LR_SCHEDULER: single_step
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  PATIENCE: 3
  POLY_LR: False
  POWER: 0.9
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: True
  STAGED_LR: False
  START_EPOCH: 0
  STEP_LR: True
  STEP_SIZE: 20
  THRESHOLD: 0.0001
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_STEPS: 1
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.002
TEST:
  EVALUATOR: CodeTalkerEvaluator
  FINAL_MODEL: last_step
  NO_TEST: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 1
  EVALUATE: True
  EVAL_FREQ: 10
  PRINT_FREQ: 5
  SAVE_FREQ: 1
  SYNC_BN: False
  USE_SGD: False
TRAINER:
  NAME: CodeTalkerTrainer
[2025-08-11 16:07:13 base_config.py line 260]=>INFO: Collecting env info ...
[2025-08-11 16:07:31 base_config.py line 261]=>INFO: Env information:
PyTorch version: 1.12.1
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 11 ¼ÒÍ¥ÖÐÎÄ°æ
GCC version: (x86_64-posix-sjlj-rev0, Built by MinGW-W64 project) 8.1.0
Clang version: Could not collect
CMake version: version 3.30.1
Libc version: N/A

Python version: 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.26100-SP0
Is CUDA available: True
CUDA runtime version: 11.3.58
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3050 Laptop GPU
Nvidia driver version: 577.00
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.22.3
[pip3] pytorch3d==0.7.2
[pip3] torch==1.12.1
[pip3] torchaudio==0.12.1
[pip3] torchvision==0.13.1
[conda] blas                      1.0                         mkl    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free
[conda] cudatoolkit               11.3.1               h59b6b97_2    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] mkl                       2023.1.0         h6b88ed4_46358    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] mkl-service               2.4.0            py39h827c3e9_2    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] mkl_fft                   1.3.11           py39h827c3e9_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] mkl_random                1.2.8            py39hc64d2fc_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] numpy                     1.22.3                   pypi_0    pypi
[conda] pytorch                   1.12.1          py3.9_cuda11.3_cudnn8_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] pytorch3d                 0.7.2                    pypi_0    pypi
[conda] torchaudio                0.12.1               py39_cu113    pytorch
[conda] torchvision               0.13.1               py39_cu113    pytorch
[2025-08-11 16:07:31 base_trainer.py line 21]=>INFO: Loading trainer: CodeTalkerTrainer
[2025-08-11 16:07:31 base_dataset.py line 17]=>INFO: Loading dataset: Vocaset
D:\anaconda3\envs\pytorch3d\lib\site-packages\huggingface_hub\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\zhli\.cache\huggingface\hub\models--facebook--wav2vec2-base-960h. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
[2025-08-11 16:08:58 base_datamanager.py line 93]=>INFO: Dataset summary:
-------  -------
Dataset  Vocaset
# train  314
# val    40
# test   39
-------  -------
[2025-08-11 16:08:58 codetalker.py line 53]=>INFO: Building model  ...
[2025-08-11 16:09:02 codetalker.py line 58]=>INFO: Params: 131,653,341
[2025-08-11 16:09:02 codetalker.py line 59]=>INFO: Model Structure:
VQAutoEncoder(
  (encoder): TransformerEncoder(
    (vertice_mapping): Sequential(
      (0): Linear(in_features=15069, out_features=1024, bias=True)
      (1): LeakyReLU(negative_slope=0.2, inplace=True)
    )
    (squasher): Sequential(
      (0): Sequential(
        (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,), padding_mode=replicate)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder_transformer): Transformer(
      (net): Sequential(
        (0): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (1): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (2): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (3): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (4): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (5): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (6): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (7): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (8): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (9): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (10): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (11): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
      )
    )
    (encoder_pos_embedding): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder_linear_embedding): LinearEmbedding(
      (net): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (decoder): TransformerDecoder(
    (expander): ModuleList(
      (0): Sequential(
        (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,), padding_mode=replicate)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (decoder_transformer): Transformer(
      (net): Sequential(
        (0): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (1): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (2): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (3): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (4): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (5): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (6): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (7): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (8): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (9): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (10): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (11): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
      )
    )
    (decoder_pos_embedding): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (decoder_linear_embedding): LinearEmbedding(
      (net): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (vertice_map_reverse): Linear(in_features=1024, out_features=15069, bias=True)
  )
  (quantize): VectorQuantizer(
    (embedding): Embedding(256, 64)
  )
)
[2025-08-11 16:09:02 codetalker.py line 61]=>INFO: Building optimizer ...
[2025-08-11 16:09:02 base_evaluator.py line 10]=>INFO: Loading evaluator: CodeTalkerEvaluator
[2025-08-11 16:09:02 base_trainer.py line 180]=>INFO: Using VQ loss function for metrics ...
[2025-08-11 16:09:02 base_trainer.py line 653]=>WARNING: No checkpoint found, train from scratch
[2025-08-11 16:09:02 base_trainer.py line 100]=>INFO: Initialize tensorboard (log_dir=./output\tensorboard)
[2025-08-11 16:09:14 codetalker.py line 151]=>INFO: epoch [1/200] batch [5/314] time 0.156 (2.381) data 0.010 (0.040) loss 0.2602 lr 1.0000e-04 eta 1 day, 17:32:19
[2025-08-11 16:09:15 codetalker.py line 151]=>INFO: epoch [1/200] batch [10/314] time 0.153 (1.274) data 0.015 (0.030) loss 0.1952 lr 1.0000e-04 eta 22:12:56
[2025-08-11 16:09:16 codetalker.py line 151]=>INFO: epoch [1/200] batch [15/314] time 0.162 (0.907) data 0.011 (0.026) loss 0.1693 lr 1.0000e-04 eta 15:49:18
[2025-08-11 16:09:17 codetalker.py line 151]=>INFO: epoch [1/200] batch [20/314] time 0.142 (0.722) data 0.008 (0.024) loss 0.1518 lr 1.0000e-04 eta 12:35:22
[2025-08-11 16:09:18 codetalker.py line 151]=>INFO: epoch [1/200] batch [25/314] time 0.152 (0.614) data 0.010 (0.023) loss 0.1404 lr 1.0000e-04 eta 10:42:09
[2025-08-11 16:09:20 codetalker.py line 151]=>INFO: epoch [1/200] batch [30/314] time 0.188 (0.542) data 0.026 (0.024) loss 0.1272 lr 1.0000e-04 eta 9:26:35
[2025-08-11 16:09:21 codetalker.py line 151]=>INFO: epoch [1/200] batch [35/314] time 0.141 (0.487) data 0.017 (0.024) loss 0.1226 lr 1.0000e-04 eta 8:29:50
[2025-08-11 16:09:22 codetalker.py line 151]=>INFO: epoch [1/200] batch [40/314] time 0.181 (0.445) data 0.012 (0.023) loss 0.1122 lr 1.0000e-04 eta 7:45:47
[2025-08-11 16:09:23 codetalker.py line 151]=>INFO: epoch [1/200] batch [45/314] time 0.150 (0.416) data 0.033 (0.023) loss 0.1073 lr 1.0000e-04 eta 7:15:05
[2025-08-11 16:09:24 codetalker.py line 151]=>INFO: epoch [1/200] batch [50/314] time 0.206 (0.391) data 0.044 (0.024) loss 0.1019 lr 1.0000e-04 eta 6:48:24
[2025-08-11 16:09:25 codetalker.py line 151]=>INFO: epoch [1/200] batch [55/314] time 0.200 (0.370) data 0.038 (0.023) loss 0.0986 lr 1.0000e-04 eta 6:27:20
[2025-08-11 16:09:26 codetalker.py line 151]=>INFO: epoch [1/200] batch [60/314] time 0.128 (0.352) data 0.010 (0.022) loss 0.0952 lr 1.0000e-04 eta 6:07:45
[2025-08-11 16:09:27 codetalker.py line 151]=>INFO: epoch [1/200] batch [65/314] time 0.145 (0.336) data 0.024 (0.022) loss 0.0913 lr 1.0000e-04 eta 5:50:58
[2025-08-11 16:09:28 codetalker.py line 151]=>INFO: epoch [1/200] batch [70/314] time 0.162 (0.323) data 0.007 (0.022) loss 0.0881 lr 1.0000e-04 eta 5:37:33
[2025-08-11 16:09:29 codetalker.py line 151]=>INFO: epoch [1/200] batch [75/314] time 0.137 (0.311) data 0.014 (0.021) loss 0.0856 lr 1.0000e-04 eta 5:25:36
[2025-08-11 16:09:30 codetalker.py line 151]=>INFO: epoch [1/200] batch [80/314] time 0.116 (0.301) data 0.013 (0.021) loss 0.0821 lr 1.0000e-04 eta 5:14:38
[2025-08-11 16:09:31 codetalker.py line 151]=>INFO: epoch [1/200] batch [85/314] time 0.170 (0.293) data 0.032 (0.021) loss 0.0808 lr 1.0000e-04 eta 5:05:55
[2025-08-11 16:09:32 codetalker.py line 151]=>INFO: epoch [1/200] batch [90/314] time 0.179 (0.285) data 0.021 (0.021) loss 0.0780 lr 1.0000e-04 eta 4:57:28
[2025-08-11 16:09:33 codetalker.py line 151]=>INFO: epoch [1/200] batch [95/314] time 0.160 (0.278) data 0.027 (0.021) loss 0.0749 lr 1.0000e-04 eta 4:50:24
[2025-08-11 16:09:34 codetalker.py line 151]=>INFO: epoch [1/200] batch [100/314] time 0.154 (0.272) data 0.031 (0.021) loss 0.0725 lr 1.0000e-04 eta 4:44:11
[2025-08-11 16:09:35 codetalker.py line 151]=>INFO: epoch [1/200] batch [105/314] time 0.135 (0.267) data 0.013 (0.020) loss 0.0706 lr 1.0000e-04 eta 4:39:02
[2025-08-11 16:09:36 codetalker.py line 151]=>INFO: epoch [1/200] batch [110/314] time 0.143 (0.261) data 0.024 (0.020) loss 0.0680 lr 1.0000e-04 eta 4:33:12
[2025-08-11 16:09:37 codetalker.py line 151]=>INFO: epoch [1/200] batch [115/314] time 0.161 (0.257) data 0.027 (0.020) loss 0.0658 lr 1.0000e-04 eta 4:28:38
[2025-08-11 16:09:38 codetalker.py line 151]=>INFO: epoch [1/200] batch [120/314] time 0.141 (0.253) data 0.013 (0.020) loss 0.0645 lr 1.0000e-04 eta 4:24:10
[2025-08-11 16:09:39 codetalker.py line 151]=>INFO: epoch [1/200] batch [125/314] time 0.131 (0.249) data 0.014 (0.020) loss 0.0623 lr 1.0000e-04 eta 4:19:57
[2025-08-11 16:09:40 codetalker.py line 151]=>INFO: epoch [1/200] batch [130/314] time 0.165 (0.246) data 0.023 (0.020) loss 0.0599 lr 1.0000e-04 eta 4:16:43
[2025-08-11 16:09:41 codetalker.py line 151]=>INFO: epoch [1/200] batch [135/314] time 0.130 (0.242) data 0.006 (0.020) loss 0.0588 lr 1.0000e-04 eta 4:13:12
[2025-08-11 16:09:42 codetalker.py line 151]=>INFO: epoch [1/200] batch [140/314] time 0.124 (0.239) data 0.010 (0.019) loss 0.0572 lr 1.0000e-04 eta 4:09:09
[2025-08-11 16:09:43 codetalker.py line 151]=>INFO: epoch [1/200] batch [145/314] time 0.167 (0.235) data 0.043 (0.020) loss 0.0556 lr 1.0000e-04 eta 4:05:38
[2025-08-11 16:09:44 codetalker.py line 151]=>INFO: epoch [1/200] batch [150/314] time 0.156 (0.233) data 0.032 (0.020) loss 0.0536 lr 1.0000e-04 eta 4:03:06
[2025-08-11 16:09:45 codetalker.py line 151]=>INFO: epoch [1/200] batch [155/314] time 0.257 (0.231) data 0.064 (0.020) loss 0.0522 lr 1.0000e-04 eta 4:00:44
[2025-08-11 16:09:46 codetalker.py line 151]=>INFO: epoch [1/200] batch [160/314] time 0.134 (0.229) data 0.012 (0.020) loss 0.0505 lr 1.0000e-04 eta 3:59:06
[2025-08-11 16:09:47 codetalker.py line 151]=>INFO: epoch [1/200] batch [165/314] time 0.140 (0.227) data 0.011 (0.020) loss 0.0494 lr 1.0000e-04 eta 3:56:35
[2025-08-11 16:09:49 codetalker.py 