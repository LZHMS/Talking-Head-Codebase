[2025-08-11 17:59:40 base_config.py line 230]=>INFO: Initializing main logger ...
[2025-08-11 17:59:40 base_config.py line 234]=>INFO: Setting fixed seed: 131
[2025-08-11 17:59:41 base_config.py line 276]=>INFO: Collecting system info ...
[2025-08-11 17:59:41 base_config.py line 277]=>INFO: Project configuration:
DATALOADER:
  NUM_WORKERS: 0
  TEST:
    BATCH_SIZE: 1
  TRAIN:
    BATCH_SIZE: 1
DATASET:
  HDTF_TFHP:
    COEF_STATS: stats_train.npz
    LMDB: 
    TEST: test.txt
    TRAIN: train.txt
    VAL: val.txt
  NAME: Vocaset
  ROOT: ./data/
  SPLIT: split.txt
  VAL_PERCENT: 0.1
  VOCASET:
    AUDIO: wav
    READ_AUDIO: True
    TEMPLATE: templates.pkl
    TEST: [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
    TRAIN: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
    VAL: [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
    VERTICES: vertices_npy
    WAV2VEC2: facebook/wav2vec2-base-960h
ENV:
  DESCRIPTION: Train the model of stage 1 on the vocaset dataset.
  GPU: 0
  NAME: train
  OUTPUT_DIR: ./output
  RESUME: 
  SEED: 131
  USE_CUDA: True
  VERBOSE: True
  VERSION: 1
INPUT:
  GN_MEAN: 0.0
  GN_STD: 0.15
  NO_TRANSFORM: False
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
LOSS:
  NAME: VQLoss
  QUANT_LOSS_WEIGHT: 1.0
MODEL:
  BACKBONE:
    FACE_QUAN_NUM: 16
    HIDDEN_SIZE: 1024
    INAFFINE: False
    INTERMEDIATE_SIZE: 1536
    IN_DIM: 15069
    NAME: stage1_vocaset
    NEG: 0.2
    NUM_ATTENTION_HEADS: 8
    NUM_HIDDEN_LAYERS: 6
    PRETRAINED: True
    QUANT_FACTOR: 0
    WINDOW_SIZE: 1
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
    N_EMBED: 256
    ZQUANT_DIM: 64
  INIT_WEIGHTS: 
  NAME: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  ADAPTIVE_LR: False
  BASE_LR_MULT: 0.1
  FACTOR: 0.3
  GAMMA: 0.5
  LR: 0.0001
  LR_SCHEDULER: single_step
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  PATIENCE: 3
  POLY_LR: False
  POWER: 0.9
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: True
  STAGED_LR: False
  START_EPOCH: 0
  STEP_LR: True
  STEP_SIZE: 20
  THRESHOLD: 0.0001
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_STEPS: 1
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.002
TEST:
  EVALUATOR: CodeTalkerEvaluator
  FINAL_MODEL: last_step
  NO_TEST: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 1
  EVALUATE: True
  EVAL_FREQ: 10
  PRINT_FREQ: 5
  SAVE_FREQ: 1
  SYNC_BN: False
  USE_SGD: False
TRAINER:
  NAME: CodeTalkerTrainer
[2025-08-11 17:59:41 base_config.py line 278]=>INFO: Collecting env info ...
[2025-08-11 17:59:53 base_config.py line 279]=>INFO: Env information:
PyTorch version: 1.12.1
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 11 ¼ÒÍ¥ÖÐÎÄ°æ
GCC version: (x86_64-posix-sjlj-rev0, Built by MinGW-W64 project) 8.1.0
Clang version: Could not collect
CMake version: version 3.30.1
Libc version: N/A

Python version: 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.26100-SP0
Is CUDA available: True
CUDA runtime version: 11.3.58
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3050 Laptop GPU
Nvidia driver version: 577.00
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.22.3
[pip3] pytorch3d==0.7.2
[pip3] torch==1.12.1
[pip3] torchaudio==0.12.1
[pip3] torchvision==0.13.1
[conda] blas                      1.0                         mkl    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free
[conda] cudatoolkit               11.3.1               h59b6b97_2    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] mkl                       2023.1.0         h6b88ed4_46358    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] mkl-service               2.4.0            py39h827c3e9_2    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] mkl_fft                   1.3.11           py39h827c3e9_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] mkl_random                1.2.8            py39hc64d2fc_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] numpy                     1.22.3                   pypi_0    pypi
[conda] pytorch                   1.12.1          py3.9_cuda11.3_cudnn8_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] pytorch3d                 0.7.2                    pypi_0    pypi
[conda] torchaudio                0.12.1               py39_cu113    pytorch
[conda] torchvision               0.13.1               py39_cu113    pytorch
[2025-08-11 17:59:53 base_trainer.py line 23]=>INFO: Loading trainer: CodeTalkerTrainer
[2025-08-11 17:59:53 base_dataset.py line 17]=>INFO: Loading dataset: Vocaset
[2025-08-11 18:00:53 base_datamanager.py line 94]=>INFO: Dataset summary:
-------  -------
Dataset  Vocaset
# train  314
# val    40
# test   39
-------  -------
[2025-08-11 18:00:53 codetalker.py line 53]=>INFO: Building model  ...
[2025-08-11 18:00:56 codetalker.py line 58]=>INFO: Params: 131,653,341
[2025-08-11 18:00:56 codetalker.py line 59]=>INFO: Model Structure:
VQAutoEncoder(
  (encoder): TransformerEncoder(
    (vertice_mapping): Sequential(
      (0): Linear(in_features=15069, out_features=1024, bias=True)
      (1): LeakyReLU(negative_slope=0.2, inplace=True)
    )
    (squasher): Sequential(
      (0): Sequential(
        (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,), padding_mode=replicate)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder_transformer): Transformer(
      (net): Sequential(
        (0): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (1): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (2): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (3): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (4): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (5): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (6): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (7): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (8): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (9): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (10): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (11): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
      )
    )
    (encoder_pos_embedding): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder_linear_embedding): LinearEmbedding(
      (net): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (decoder): TransformerDecoder(
    (expander): ModuleList(
      (0): Sequential(
        (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,), padding_mode=replicate)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (decoder_transformer): Transformer(
      (net): Sequential(
        (0): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (1): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (2): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (3): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (4): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (5): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (6): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (7): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (8): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (9): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (10): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (11): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
      )
    )
    (decoder_pos_embedding): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (decoder_linear_embedding): LinearEmbedding(
      (net): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (vertice_map_reverse): Linear(in_features=1024, out_features=15069, bias=True)
  )
  (quantize): VectorQuantizer(
    (embedding): Embedding(256, 64)
  )
)
[2025-08-11 18:00:56 codetalker.py line 61]=>INFO: Building optimizer ...
[2025-08-11 18:00:56 base_evaluator.py line 12]=>INFO: Loading evaluator: CodeTalkerEvaluator
[2025-08-11 18:00:56 base_trainer.py line 181]=>INFO: Using VQ loss function for metrics ...
[2025-08-11 18:00:56 base_trainer.py line 654]=>WARNING: No checkpoint found, train from scratch
[2025-08-11 18:00:56 base_trainer.py line 101]=>INFO: Initialize tensorboard (log_dir=./output\tensorboard)
fatal   : Memory allocation failure
fatal   : Memory allocation failure
Traceback (most recent call last):
  File "D:\anaconda3\envs\pytorch3d\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "D:\anaconda3\envs\pytorch3d\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "F:\Workspace\Projects\TalkingHeadCodebase\main\train.py", line 42, in <module>
    main(args)
  File "F:\Workspace\Projects\TalkingHeadCodebase\main\train.py", line 28, in main
    trainer.train()
  File "F:\Workspace\Projects\TalkingHeadCodebase\trainers\codetalker.py", line 72, in train
    super().train(self.start_epoch, self.max_epoch)
  File "F:\Workspace\Projects\TalkingHeadCodebase\base\base_trainer.py", line 134, in train
    self.run_epoch()
  File "F:\Workspace\Projects\TalkingHeadCodebase\trainers\codetalker.py", line 121, in run_epoch
    loss_details, info = self.forward_backward(batch)
  File "F:\Workspace\Projects\TalkingHeadCodebase\trainers\codetalker.py", line 179, in forward_backward
    self.model_backward_and_update(loss)
  File "F:\Workspace\Projects\TalkingHeadCodebase\base\base_trainer.py", line 212, in model_backward_and_update
    self.model_backward(loss)
  File "F:\Workspace\Projects\TalkingHeadCodebase\base\base_trainer.py", line 195, in model_backward
    loss.backward()
  File "D:\anaconda3\envs\pytorch3d\lib\site-packages\torch\_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "D:\anaconda3\envs\pytorch3d\lib\site-packages\torch\autograd\__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
You can try to repro this exception using the following code snippet. If that doesn't trigger the error, please include your original repro script when reporting this issue.

import torch
torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False
torch.backends.cudnn.allow_tf32 = True
data = torch.randn([1, 1024, 1, 150], dtype=torch.float, device='cuda', requires_grad=True)
net = torch.nn.Conv2d(1024, 1024, kernel_size=[1, 5], padding=[0, 0], stride=[1, 1], dilation=[1, 1], groups=1)
net = net.cuda().float()
out = net(data)
out.backward(torch.randn_like(out))
torch.cuda.synchronize()

ConvolutionParams 
    memory_format = Contiguous
    data_type = CUDNN_DATA_FLOAT
    padding = [0, 0, 0]
    stride = [1, 1, 0]
    dilation = [1, 1, 0]
    groups = 1
    deterministic = false
    allow_tf32 = true
input: TensorDescriptor 000002BC65031820
    type = CUDNN_DATA_FLOAT
    nbDims = 4
    dimA = 1, 1024, 1, 150, 
    strideA = 153600, 150, 150, 1, 
output: TensorDescriptor 000002BC650310B0
    type = CUDNN_DATA_FLOAT
    nbDims = 4
    dimA = 1, 1024, 1, 146, 
    strideA = 149504, 146, 146, 1, 
weight: FilterDescriptor 000002BBD6BA0F60
    type = CUDNN_DATA_FLOAT
    tensor_format = CUDNN_TENSOR_NCHW
    nbDims = 4
    dimA = 1024, 1024, 1, 5, 
Pointer addresses: 
    input: 0000000782124000
    output: 0000000781E00000
    weight: 000000078D800000

