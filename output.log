[2025-08-29 17:01:04 base_config.py line 238]=>INFO: Initializing main logger ...
[2025-08-29 17:01:04 base_config.py line 242]=>INFO: Setting fixed seed: 131
[2025-08-29 17:01:04 base_config.py line 284]=>INFO: Collecting system info ...
[2025-08-29 17:01:04 base_config.py line 285]=>INFO: Project configuration:
DATALOADER:
  NUM_WORKERS: 0
  TEST:
    BATCH_SIZE: 1
  TRAIN:
    BATCH_SIZE: 1
DATASET:
  HDTF_TFHP:
    AUDIO_SR: 16000
    COEF_FPS: 25
    COEF_STATS: stats_train.npz
    CROP: random
    LMDB: 
    MOTIONS: 100
    ROT_REPR: aa
    TEST: test.txt
    TRAIN: train.txt
    VAL: val.txt
  NAME: HDTF_TFHP
  ROOT: ./data/
  SPLIT: split.txt
  VAL_PERCENT: 0.1
  VOCASET:
    AUDIO: wav
    READ_AUDIO: True
    TEMPLATE: templates.pkl
    TEST: [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
    TRAIN: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
    VAL: [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
    VERTICES: vertices_npy
    WAV2VEC2: facebook/wav2vec2-base-960h
ENV:
  DESCRIPTION: Train the model of stage 1 on the vocaset dataset.
  GPU: 0
  NAME: train
  OUTPUT_DIR: ./output
  RESUME: 
  SEED: 131
  USE_CUDA: True
  VERBOSE: True
  VERSION: 1
INPUT:
  GN_MEAN: 0.0
  GN_STD: 0.15
  NO_TRANSFORM: False
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
LOSS:
  NAME: VQLoss
  QUANT_LOSS_WEIGHT: 1.0
MODEL:
  BACKBONE:
    FACE_QUAN_NUM: 16
    HIDDEN_SIZE: 1024
    INAFFINE: False
    INTERMEDIATE_SIZE: 1536
    IN_DIM: 15069
    NAME: stage1_vocaset
    NEG: 0.2
    NUM_ATTENTION_HEADS: 8
    NUM_HIDDEN_LAYERS: 6
    PRETRAINED: True
    QUANT_FACTOR: 0
    WINDOW_SIZE: 1
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
    N_EMBED: 256
    ZQUANT_DIM: 64
  INIT_WEIGHTS: 
  NAME: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  ADAPTIVE_LR: False
  BASE_LR_MULT: 0.1
  FACTOR: 0.3
  GAMMA: 0.5
  LR: 0.0001
  LR_SCHEDULER: single_step
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  PATIENCE: 3
  POLY_LR: False
  POWER: 0.9
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: True
  STAGED_LR: False
  START_EPOCH: 0
  STEP_LR: True
  STEP_SIZE: 20
  THRESHOLD: 0.0001
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_STEPS: 1
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.002
TEST:
  EVALUATOR: CodeTalkerEvaluator
  FINAL_MODEL: last_step
  NO_TEST: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 1
  EVALUATE: True
  EVAL_FREQ: 10
  PRINT_FREQ: 5
  SAVE_FREQ: 1
  SYNC_BN: False
  USE_SGD: False
TRAINER:
  NAME: CodeStyle
[2025-08-29 17:01:04 base_config.py line 286]=>INFO: Collecting env info ...
[2025-08-29 17:01:10 base_config.py line 287]=>INFO: Env information:
PyTorch version: 1.12.1
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 11 ×¨Òµ°æ
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Libc version: N/A

Python version: 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.26100-SP0
Is CUDA available: True
CUDA runtime version: 12.4.131
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090 D
Nvidia driver version: 560.94
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] facenet-pytorch==2.3.0
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.23.0
[pip3] pytorch3d==0.7.2
[pip3] torch==1.12.1
[pip3] torchaudio==0.12.1
[pip3] torchvision==0.13.1
[conda] blas                            1.0                    mkl                      https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free
[conda] cudatoolkit                     11.3.1                 h59b6b97_2               https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] mkl                             2023.1.0               h6b88ed4_46358           https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] mkl-service                     2.4.0                  py39h827c3e9_2           https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] mkl_fft                         1.3.11                 py39h827c3e9_0           https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] mkl_random                      1.2.8                  py39hc64d2fc_0           https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] numpy                           1.23.0                 pypi_0                   pypi
[conda] numpy-base                      1.26.4                 py39h65a83cf_0           https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
[conda] pytorch                         1.12.1                 py3.9_cuda11.3_cudnn8_0  pytorch
[conda] pytorch-mutex                   1.0                    cuda                     pytorch
[conda] pytorch3d                       0.7.2                  pypi_0                   pypi
[conda] torchaudio                      0.12.1                 py39_cu113               pytorch
[conda] torchvision                     0.13.1                 py39_cu113               pytorch
[2025-08-29 17:01:10 base_trainer.py line 23]=>INFO: Loading trainer: CodeStyle
[2025-08-29 17:01:10 base_dataset.py line 17]=>INFO: Loading dataset: HDTF_TFHP
[2025-08-29 17:01:13 base_datamanager.py line 91]=>INFO: Dataset summary:
-------  ---------
Dataset  HDTF_TFHP
# train  875
# val    118
# test   120
-------  ---------
[2025-08-29 17:01:13 codestyle.py line 54]=>INFO: Building model  ...
[2025-08-29 17:01:14 codestyle.py line 59]=>INFO: Params: 131,653,341
[2025-08-29 17:01:14 codestyle.py line 60]=>INFO: Model Structure:
StyleVQAE(
  (encoder): TransformerEncoder(
    (vertice_mapping): Sequential(
      (0): Linear(in_features=15069, out_features=1024, bias=True)
      (1): LeakyReLU(negative_slope=0.2, inplace=True)
    )
    (squasher): Sequential(
      (0): Sequential(
        (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,), padding_mode=replicate)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder_transformer): Transformer(
      (net): Sequential(
        (0): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (1): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (2): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (3): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (4): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (5): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (6): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (7): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (8): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (9): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (10): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (11): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
      )
    )
    (encoder_pos_embedding): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder_linear_embedding): LinearEmbedding(
      (net): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (decoder): TransformerDecoder(
    (expander): ModuleList(
      (0): Sequential(
        (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,), padding_mode=replicate)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (decoder_transformer): Transformer(
      (net): Sequential(
        (0): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (1): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (2): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (3): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (4): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (5): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (6): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (7): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (8): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (9): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
        (10): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)
              (to_out): Linear(in_features=1024, out_features=1024, bias=True)
              (rearrange_qkv): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=8)
              (rearrange_out): Rearrange('b h n d -> b n (h d)')
            )
          )
        )
        (11): Residual(
          (fn): Norm(
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fn): MLP(
              (l1): Linear(in_features=1024, out_features=1536, bias=True)
              (l2): Linear(in_features=1536, out_features=1024, bias=True)
            )
          )
        )
      )
    )
    (decoder_pos_embedding): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (decoder_linear_embedding): LinearEmbedding(
      (net): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (vertice_map_reverse): Linear(in_features=1024, out_features=15069, bias=True)
  )
  (quantize): VectorQuantizer(
    (embedding): Embedding(256, 64)
  )
)
[2025-08-29 17:01:14 codestyle.py line 62]=>INFO: Building optimizer ...
[2025-08-29 17:01:14 base_evaluator.py line 12]=>INFO: Loading evaluator: CodeTalkerEvaluator
[2025-08-29 17:01:14 base_trainer.py line 181]=>INFO: Using VQ loss function for metrics ...
[2025-08-29 17:01:14 base_trainer.py line 654]=>WARNING: No checkpoint found, train from scratch
[2025-08-29 17:01:14 base_trainer.py line 101]=>INFO: Initialize tensorboard (log_dir=./output\tensorboard)
Traceback (most recent call last):
  File "D:\Software\Miniconda3\envs\pytorch3d\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "D:\Software\Miniconda3\envs\pytorch3d\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "D:\Projects\TalkingHeadCodebase\main\train.py", line 42, in <module>
    main(args)
  File "D:\Projects\TalkingHeadCodebase\main\train.py", line 28, in main
    trainer.train()
  File "D:\Projects\TalkingHeadCodebase\trainers\codestyle.py", line 73, in train
    super().train(self.start_epoch, self.max_epoch)
  File "D:\Projects\TalkingHeadCodebase\base\base_trainer.py", line 134, in train
    self.run_epoch()
  File "D:\Projects\TalkingHeadCodebase\trainers\codestyle.py", line 122, in run_epoch
    loss_details, info = self.forward_backward(batch)
  File "D:\Projects\TalkingHeadCodebase\trainers\codestyle.py", line 174, in forward_backward
    output, quant_loss, info = self.model(coefficients)
  File "D:\Software\Miniconda3\envs\pytorch3d\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\Projects\TalkingHeadCodebase\models\StyleVQAE.py", line 20, in forward
    quant, emb_loss, info = self.encode(x)
  File "D:\Projects\TalkingHeadCodebase\models\StyleVQAE.py", line 10, in encode
    h = self.encoder(x) ## x --> z'
  File "D:\Software\Miniconda3\envs\pytorch3d\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\Projects\TalkingHeadCodebase\models\vqae.py", line 178, in forward
    inputs = self.vertice_mapping(inputs)
  File "D:\Software\Miniconda3\envs\pytorch3d\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\Software\Miniconda3\envs\pytorch3d\lib\site-packages\torch\nn\modules\container.py", line 139, in forward
    input = module(input)
  File "D:\Software\Miniconda3\envs\pytorch3d\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\Software\Miniconda3\envs\pytorch3d\lib\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (200x156 and 15069x1024)
